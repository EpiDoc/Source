<!-- Start license statement: do not remove 

EpiDoc: Guidelines for Structured Markup of Epigraphic Texts in TEI
Copyright (C) 2000-2006 by all contributors listed in <div type="gl-responsibility">, below.
Additional contributors' copyright may be designated in individual source files.

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; either version 2
of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

Information about the EpiDoc community can be obtained via 
http://epidoc.sf.net.

End license statement: do not remove -->
<!-- $Header: https://epidoc.svn.sourceforge.net/svnroot/epidoc/trunk/guidelines/xml-p5/xml/introeps.xml 1159 2010-05-14 11:02:59Z paregorios $ -->
<?oxygen NVDLSchema="../schema/epidoc.nvdl"?>

<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="intro-eps-fr" rend="integral" xml:lang="fr">
    <head>Introduction pour les épigraphistes</head>
    <p>Les principes sous tendant EpiDoc rassemblent, pour les épigraphistes, des concepts traditionnels ainsi que des méthodes d'édition et des conventions tout à fait 
	nouvelles.</p>
    <div xml:id="IntroEpigraphers-conventions">
        <head>Conventions épigraphiques</head>
        <p> Au cours du siècle dernier, les épigraphistes ont été confrontés aux problèmes liés à la représentation d'informations non verbales dans leurs textes 
			écrits. 
			Les éditeurs ont, jusqu'à la fin du 19ème siècle, produit des facsimile des textes mais cette pratique devint ensuite de plus en plus rare, 
			les éditeurs montrant en outre peu d'empressement à remplacer ces facsimile par des reproductions photographiques intégrales de chaque document. 
			Des conventions furent par ailleurs (et non sans mal) développées pour représenter les zones de texte manquantes, les abréviations, etc., 
			et sont globalement acceptées par tous depuis les années 1930, partageant nombre de points communs avec celles développées en papyrologie et 
			en paléographie. 
			Tous les épigraphistes ont depuis été confrontés aux conséquences du passage à l'informatique, notamment la difficulté de reproduire ces conventions, 
			notamment la difficulté de trouver une police permettant de mettre un point sous un caractère. Cependant la plupart ont désormais surmonté ces obstacles. 
		</p>
        <p>
			Les difficultés posées par ces questions, en particulier le rendu des caractères grecs avec des polices complètes et affichables sur le web, 
			ont généralement ralentit la publication en ligne de textes épigraphiques. En lieu et place sont apparues des bases de données d'une grande richesse, 
			parmi lesquelles :
		
			<list type="unordered">
                <item>L'<ref target="http://www.eagle-eagle.it">Electronic Archive of Greek and Latin Epigraphy (EAGLE)</ref> et les bases de données qui la composent, projet mené sous la conduite de l'Association Internationale d'Epigraphie Grecque et Latine (AIEGL): 
					<list type="unordered">
                        <item>
                            <ref target="http://www.edb.uniba.it/">Epigraphic Database Bari (EDB)</ref>
                        </item>
                        <item>
                            <ref target="http://www.uni-heidelberg.de/institute/sonst/adw/edh/" xml:lang="de">Epigraphische Datenbank Heidelberg (EDH)</ref>
                        </item>
                        <item>
                            <ref target="http://www.edr-edr.it/">Epigraphic Database Roma (EDR)</ref>
                        </item>
                    </list>
                </item>
                <item>La collection de textes grecs du Packard Humanities Institute <ref target="http://epigraphy.packhum.org/inscriptions/" n="PHI Greek Epigraphic Texts Online">disponible en ligne</ref> depuis 2005.</item>
            </list>
        </p>
        <p>Se référer également à: <ref target="conformance.xml">Compatibilité d'EpiDoc</ref>.</p>
		
        <p>
			Tous ces développements ont en outre été déterminés par l'état de l'art technologique au moment de leur mise en œuvre, l'informatique ayant connu
			d'importantes évolutions au cours du 20ème siècle. L'objectif d'EpiDoc est d'exploiter ces nouvelles technologies pour poursuivre les objectifs 
			traditionnels de l'épigraphie par de nouveaux moyens. 
			Nombre des processus évoqués plus haut ont impliqué de lutter contre des limitations technologiques (notamment dans le domaine de l'impression papier)
			afin de répondre au plus grand nombre de besoins des épigraphistes. Avec le temps il est devenu de plus en plus difficile de persuader les imprimeurs 
			classiques d'accepter les besoins spécifiques d'insertion d'information méta-textuelles, sauf à payer des tarifs exorbitants. 
			Et ce alors même que la quantité d'information accompagnant chaque texte a grandement augmenté : outre les informations sur l'apparence physique ou 
			les circonstances de découverte, la représentation photographique des documents étant également standard. 
		</p>
        <p>
			Les quinze dernières années ont vu les chercheurs affronter des problèmes généralement similaires en terme d'intégration de métadonnées au sein 
			de leurs textes en format électronique et divers outils ont rendu ces tâches de plus en plus aisée et les résultats de plus en plus bénéfiques.
			Les logiciels de traitement de texte familiers depuis les années 1980 nous permettent de contrôler l'apparence des textes en faisant usage d'un balisage 
			qui de nos jour est invisible pour l'utilisateur, masqué dans le code du fichier par le programme. 
			Les demandes plus importantes de projets de grande échelle (documents juridiques, documentation industrielle, publications commerciales, ...) ont conduit, 
			dans les années 80, à la recherche de moyens permettant d'encoder plus d'informations et d'instructions au sein des documents électroniques. 
			Au début l'attention s'est portée sur l'insertion d'informations de mise en page mais rapidement apparurent	des moyens permettant d'inclure des 
			informations sémantiques plus complexes relatives à la structure même des documents et à leur contenu.
			Un exemple simple consiste à indiquer que le titre d'un ouvrage est un titre, plutôt que de simplement le mettre en italique. 
			L'utilisation de ce balisage plus abstrait a permis une séparation de la structure et de la présentation d'un document, où la structure est propre 
			à un genre de document alors que la présentation peut varier en fonction de la méthode de publication. 
			D'une certaine façon ce basculement est une forme de retour à une époque antérieure où les auteurs s'occupaient du contenu et où la question de la 
			présentation relevait exclusivement du processus de publication, une distinction perdue lors de l'arrivée des systèmes de reproduction photographique. 	
		</p>
        <p>
			Les protocoles issus de ces efforts furent standardisés fin des années 80 en tant que "Standard Generalized Markup Language" (SGML) et furent plus récemment
			adaptés de manière à les rendre plus simples et plus souples afin de les rendre utilisables pour le web, sous le nom de XML : le <ref target="http://www.w3.org/XML/">Extensible Markup Language</ref>. 
			XML aujourd'hui largement utilisé par la communauté scientifique dans un grand nombre de disciplines des sciences humaines afin de représenter et préserver 
			du matériel de recherche pour de multiples usages.
		</p>
        <p>
			L'attrait de XML pour les épigraphistes est, de ce fait, considérable. Ainsi les lacunes peuvent être indiquées comme telles, et présentées au sein de 
			crochets droits.
			Simultanément une recherche spécifiant que seul le texte marqué comme présent sur le document original doit être pris en compte peut être menée sur 
			le corpus (et donc ne prendre en compte que les mots effectivement présents, indépendamment des reconstructions).
			Les lettres à la lecture incertaines peuvent être représentées comme telles et une décision quant à leur rendu lors de l'impression peut être prise 
			ultérieurement (assurant leur identification par un point sous la lettre ou toute autre méthode souhaitée). 
			Les mots peuvent également être lemmatisés durant l'édition, permettant de créer des indices qui se complèteront automatiquement
			au fur et à mesure de l'édition du corpus.
			Ce qui est important, en revanche, est de recréer un équivalent électronique des conventions de Leyden, c'est à dire de définir un standard pour représenter
			électroniquement les différents sigles en usage. Tout d'abord cela est utile pour économiser du temps et s'éviter divers soucis. 
			Mais une telle standardisation a aussi (sans imposer une totale uniformité) une valeur intrinsèque : non seulement cela fournit une information au lecteur, 
			tout comme les sigles imprimés sur les pages papiers, mais en outre cela permet une publication électronique permettant l'exploitation simultanée
			de plusieurs corpus même si ils ont été préparés par des équipes différentes. 
		</p>
        <p>
			Ce besoin de standards repandus n'est pas limité au domaine de l'épigraphie. Depuis 1987 un groupe international de chercheurs, principalement issus du 
			domaine des sciences humaines, travaille à développer et améliorer un jeu de consignes ("guidelines") pour décrire la structure et le contenu de documents. 
			Le résultat de cet effort a été la naissance d'un langage d'encodage, produit en XML et appelé du nom du groupe qui lui a donné naissance : TEI, pour <ref target="http://www.tei-c.org/">Text Encoding Initiatve</ref>.
		</p>
    </div>
    <div xml:id="IntroEpigraphers-TEI">
        <head>TEI pour les épigraphistes : De quoi s'agit-il ? Pourquoi l'utiliser ?</head>
        <p>
			La "Text Encoding Initiative" (TEI) est un effort scientifique visant à la définition d'un langage d'encodage permettant de subvenir de façon générale aux besoins 
			des chercheurs en sciences humaines. Deux objectifs principaux ont conduit le développement de la TEI : le premier est de fournir aux chercheurs le moyen de 
			représenter leur matériel de recherche dans un format numérique en utilisant un langage descriptif qui reproduit les termes et les concepts d'analyse familiers et 
			essentiels à la recherche en sciences humaines. Le second objectif est de permettre au chercheur de partager le produit de cet encodage de façon intelligible grâce 
			à l'utilisation d'un langage descriptif commun. 
		</p>
        <p>
			On peut considérer le langage d'encodage TEI comme un langage humain : un noyau de termes partagés au centre, entourés par une série de vocabulaires
			à la diffusion plus restreinte, en ce compris des variantes locales, des termes spécifiques à certains domaines et d'autres variations.
			Au coeur du TEI se trouvent les termes communs et les concepts largement partagés par les chercheurs dans la plupart des disciplines : des éléments 
			comme les paragraphes, les divisions courantes du texte, les chapitres, les listes, etc. 
			Des éléments plus spécifiques sont regroupés en fonction de leur utilisation : ainsi les éléments pour l'encodage détaillé de noms, de caractéristiques
			de manuscrits, pour capturer la structure de dictionnaires, et ainsi de suite.
			Le TEI est volontairement construit ainsi, sous forme de modules, afin que les chercheurs actifs dans différentes disciplines puissent utiliser les
			seuls modules pertinents à leur recherche. La TEI permet ainsi d'offrir une grande liberté sans surcharger les chercheurs individuels et les projets 
			plus larges avec le besoin d'apprendre nombre de connaissances sortant de leur domaine, la plupart desquelles n'étant relevant que pour d'autres 
			disciplines. Au contraire, un encodage TEI peut être dirigé de manière très spécifique à un certain domaine ou une certaine tâche, et peut être limité
			à ce qui est essentiel à la réussite d'un projet. 
		</p>
        <p>	
			Tout comme un langage humain, le langage TEI peut être utilisé de manière à faire appel à un vocabulaire riche et nuancé, avec un encodage détaillé qui décrit
			une grande variété de phénomènes textuels, mais il peut aussi être utilisé de manière très simple, en se servant uniquement de quelques concepts
			essentiels qui ne fournissent qu'une information basique sur les éléments textuels : sections, chapitres, paragraphes. Le plus détaillé l'encodage, 
			le plus de possibilités de traitement mais des facteurs comme le temps disponible, le coût, le personnel assignable ou l'expertise locale peuvent être
			sources de limitations dans le niveau de détails encodable.
		</p>
        <p>	
			En plus de fournir un système d'encodage que les chercheurs peuvent utiliser dans son état standard, le TEI fournit également les outils permettant aux
			projets de recherche la définition d'une variante adaptée à leurs besoins du langage, laquelle contiendra l'ensemble des modifications nécessaires au 
			support de besoins locaux spécifiques.
			Parce que ces versions localisées du langage TEI oeuvrent au sein du cadre général, il n'est pas nécessaire de réinventer le noyau commun : les concepts
			et termes généraux restent d'application et évitent d'avoir à les réinventer.
			En outre, comme la TEI fournit un cadre commun pour créer et décrire ces modifications spécifiques, elles peuvent être aisément partagées et réutilisées. 
			De cette manière des groupes de chercheurs dans des disciplines différentes peuvent exprimer les méthodes et objectifs spécifiques à leurs disciplines et
			définir clairement ce qui les différencie des autres domaines proches. Au lieu d'avoir des approches complètement incompatibles, différents projets peuvent
			produire des résultats dont les différences sont le fruit de vraies oppositions et non de divergences aléatoires. 
		</p>
        <p/>
    </div>
    <div xml:id="IntroEpigraphers-epidoc">
        <head>The EpiDoc Customization: TEI for Epigraphers</head>
        <p>
			Au sein de ce cadre général, la <ref target="http://epidoc.sf.net">Communauté EpiDoc</ref> travaille depuis l'an 2000 à développer une version spécifique
			des règles TEI adaptées aux besoins particuliers des épigraphistes. L'idée fut lancée par <ref target="http://www.unc.edu/~thomase/">Tom Elliott</ref>, 
			historien de l'Antiquité à l'Université de Caroline du Nord (campus de Chapel Hill). L'objectif est à la fois de faire la plus grande utilisation possible
			du travail déjà effectué et de veiller à ce que la manière dont la documentation épigraphique est traitée soit la plus proche possible de la manière dont
			sont traités les autres textes, plutôt que de s'éloigner de ces méthodes.
			EpiDoc élimine ainsi les éléments non pertinents de la TEI tout en introduisant les éléments nécessaires pour que les transcriptions, l'analyse, 
			la description et la classification qui sont essentiels au travail de l'épigraphiste puisse se faire. 
			Le résultat est un langage à la fois simple et puissant qui peut être utilisé pour marquer l'ensemble des caractéristiques d'une inscription et représenter
			l'information pertinente à l'objet porteur de l'inscription. 
		</p>
        <p>
			Afin d'accompagner le language EpiDoc, la communauté EpiDoc a créé un jeu de <ref target="guidelines.xml">"guidelines"</ref> et de 
			<ref target="epidev.xml">logiciels</ref>, ainsi qu'une <ref target="http://epidocumentation.pbwiki.com">documentation</ref> expliquant comment utiliser
			le langage, les outils et tous les autres éléments constitutifs de la "méthode EpiDoc".
			L'objectif est de créer un cadre qu'il sera aisé d'apprendre et d'utiliser, y compris pour des chercheurs n'ayant aucune expérience technique ou manquant
			d'appuis.
			Cela peut sembler improbable, mais la tâche est en réalité similaire à l'apprentissage du marquage d'un texte à l'aide des conventions déjà en usage pour 
			la description de textes épigraphiques.
		</p>
        <p>
			Le groupe a travaillé pour définir des moyens de représenter l'ensemble des conventions épigraphiques en usage. Ils ont en outre élargi le présent manuel afin de 
			traiter de l'ensemble des champs susceptibles d'être présents dans une publication épigraphique, en ce compris :
			<list>
                <item>
                    <ref target="taggingtext.xml">L'édition du texte épigraphique en lui même</ref>
                </item>
                <item>
                    <ref target="describing.xml">La description du texte, de son support et de son contexte</ref>
                </item>
                <item><ref target="translation.xml">Une traduction du texte</ref></item>
                <item><ref target="commentary.xml">Un commentaire scientifique</ref> du texte, de ses difficultés et de ce qui fait son attrait</item>
                <item><ref target="history.xml">Une histoire de la découverte, de l'historiographie et de l'interprétation du texte</ref> jusqu'à la présente édition</item>
                <item><ref target="bibliographicreferences.xml">Une bibliographie</ref> pertinente (xref)</item>
            </list> Voir également: <ref target="documentstructure.xml">Structure du document</ref>. </p>
        <p>
			D'autres domaines présentement à l'étude sont, notamment, ceux relatifs au développement de l'interopérabilité. Un outil de conversion automatique de texte
			en document au format XML EpiDoc a déjà été développé : le <ref target="chetc.xml">Chapel Hill Electronic Text Converter(CHETC)</ref>.
			D'autres travaux visent à permettre l'utilisation de lexiques de contrôle. Ainsi le projet Inscriptions of Aphrodisias travaille de manière rapprochée avec le
			<ref target="http://www.lgpn.ox.ac.uk/">Lexicon of Greek Personal Names</ref> afin d'assurer une documentation exaustive et conforme aux normes en usage. 	
		</p>
        <p>
			Ce travail, mené sous la conduite du Dr. Elliott, a été entrepris par une série de chercheurs travaillant de manière concertée et veillant à rester régulièrement
			en contact avec le reste de la profession. L'équipe bénéficie également de l'expérience d'un projet EpiDoc bien établit, les <ref target="http://vindolanda.csad.ox.ac.uk/exhibition/">Vindolanda Tablets on line</ref>, 
			et de deux projets actuellement en cours, l'<ref target="http://dev.stg.brown.edu/projects/USEpigraphy/">US Epigraphy Project (USEP)</ref> (avec l'appui des universités de
			Brown, Princeton and Rutgers), et le <ref target="http://insaph.kcl.ac.uk">Inscriptions of Aphrodisias Project (InsAph)</ref> (avec l'appui de l'<ref target="http://www.ahrc.ac.uk">Arts and Humanities Research Council</ref>). 
			Ce généreux support de l'AHRC a également permit la tenue d'un <ref target="sprintsandstorm.xml">atelier de réflexion intensif en 2006</ref> durant lequel les
			présentes "guidelines" purent être réorganisées.
		</p>
    </div>
    <div xml:id="IntroEpigraphers-responsibility" type="gl-responsibility">
        <head>Responsables de cette section</head>
        <listBibl>
            <bibl>
                <respStmt>
                    <resp>Auteur</resp>
                    <name>Charlotte Roueché</name>
                </respStmt>
            </bibl>
            <bibl>
                <respStmt>
                    <resp>Auteur</resp>
                    <name>Julia Flanders</name>
                </respStmt>
            </bibl>
			<bibl>
                <respStmt>
                    <resp>Traduction française</resp>
                    <name>Pascal Lemaire</name>
                </respStmt>
            </bibl>            
			<bibl>
                <respStmt>
                    <resp>Rendu via TEI-Lite et diverses opérations de mise en page</resp>
                    <name>Tom Elliott</name>
                </respStmt>
            </bibl>
        </listBibl>
    </div>
</div>
