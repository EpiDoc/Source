<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:tei="http://www.tei-c.org/ns/1.0" xml:lang="en" lang="en"><head><title>Introduction for Epigraphers</title><style media="screen" type="text/css"> @import "webscreen.css"; </style></head><body><div id="container"><div id="main"><h1><span class="text">Introduction for Epigraphers</span></h1><div id="introduction"><h2>Introduction for Epigraphers</h2><p>The concepts behind EpiDoc bring together, for epigraphers, traditional and entirely modern editorial methods and conventions.</p></div><div id="IntroEpigraphers-conventions">
        <h2>Epigraphic conventions</h2>
        <p> Over the last century, epigraphers have wrestled with the issues involved in representing non-verbal information within their written
            texts. Until the end of the 19th century publishers could be expected to produce a facsimile of the text, but this became decreasingly
            common, and publishers did not demonstrate a parallel willingness to provide a full photographic record of every text. The conventions
            which have been painfully developed to indicate missing text, abbreviations, etc. have been more or less generally agreed since the 1930s
            and overlap, to some extent, with those used in papyrology and palaeography. All epigraphers have had to deal with the issues involved in
            moving this to an electronic environment - for example, finding a font which will permit underdotting; but most of us have now adjusted to
            these new constraints.</p>
        <p>The difficulty of rendering such conventions and, in particular, Greek characters in consistent fonts and on the Web has tended to delay
            the publication of full epigraphic texts online; instead, enormously rich search collections have been created, most notably: <ul>
                <li>the Electronic Archive of Greek and Latin Epigraphy (EAGLE) constituent databases: <ul>
                        <li>
                            <a href="http://www.edb.uniba.it/">Epigraphic Database Bari (EDB)</a>
                        </li>
                        <li>
                            <a href="http://www.uni-heidelberg.de/institute/sonst/adw/edh/" lang="de" xml:lang="de">Epigraphische Datenbank Heidelberg
                            (EDH)</a>
                        </li>
                        <li>
                            <a href="http://www.edr-edr.it/">Epigraphic Database Roma (EDR)</a>
                        </li>
                    </ul>
                </li>
                <li>the Packard Humanities Institue collection of Greek texts, <a href="http://epigraphy.packhum.org/inscriptions/">available online</a> since 2005.</li>
            </ul>
        </p>
        <p>All these developments have therefore been determined by the state of the existing technologies, as these have evolved over the 20th
            century. The object of EpiDoc is to exploit new and rich technologies for the traditional purposes of epigraphy. Many of the processes
            described above have involved struggling agaist the technological standards - for example, in print publications - in order to accommodate
            as many of our requirements as possible. Over the period it has become steadily more difficult to persaude conventional publishers to meet
            our requirements for inserting meta-textual information, unless at prohibitive expense. At the same time, the expectations as to the
            volume of information which should accompany a text have risen greatly; as well as information about physical circumstances, photographic
            illustration has become standard.</p>
        <p>In the last 15 years scholars generally have been dealing with similar requirements to incorporate meta-data within texts in their
            electronic form, and tools have emerged which make this increasingly easy and make the results increasingly valuable. The word-processing
            software that has been familiar since the 1980s allows us to control the formatting of our texts, using markup which by now is invisibly
            embedded by the software. The more demanding requirements of large-scale document collections - legal papers, industrial documentation,
            commercial publishing - led in the 1980s to the investigation of ways to insert a wider range of information and instructions within
            electronic texts. At first the emphasis was on inserting formatting instructions, but there soon emerged methods of including more complex
            semantic information concerning document structure and even content. A simple example is marking up a book title as a title, rather than
            simply marking it as being italicized. The use of this more abstract markup permits a separation of structure and presentation, where
            structure is comparatively fundamental to the document's genre, while presentation may be varied depending on the form of publication. In
            a way, this shift represents a return to an earlier mode in which authors dealt with the substance of the text and all details of
            presentation were handled in the publishing process - a distinction which has been lost in the days of camera-ready copy.</p>
        <p>The protocols which emerged from this latter effort were standardized in the late 1980s as the Standard Generalized Markup Language, and
            more recently were given a simpler and more flexible form for use in the world-wide web as XML: the <a href="http://www.w3.org/XML/">Extensible Markup Language</a>. XML is now widely used by scholars in a broad range of humanistic disciplines to capture/represent
            and preserve research materials for a wide range of purposes.</p>
        <p>The attractions of XML for epigraphers are therefore considerable. For example: missing material can be marked up as such, and then
            presented within square brackets; at the same time, a search can be instructed only to interrogate text which has not been marked up as
            missing (so only definitely attested terms). Uncertain letters can be marked up as such, and a decision made later as to whether to render
            them with an underdot or in another way. Words can be lemmatised during editing, to create indices which grow as the collection grows.
            What is important, however, at this juncture is to repeat the 'Leyden' exercise; that is, to agree electronic equivalents of the various
            sigla that we use. Firstly, this is valuable simply in order to save time and trouble; but also consistency - without imposing uniformity
            - continues to be valuable. Not only does it support the user, as on the printed page; but documents edited in this way and published
            electronically will then be exploitable together, even if they have been prepared separately.</p>
        <p>The need for agreed standards is not limited to epigraphy. Since 1987 an international consortium of scholars principally in the humanities
            has been working together to develop and refine a set of guidelines for describing the structure and content of documents. The results of
            this endeavour have produced an encoding language, realized in XML and described by the name of the group - TEI, the <a href="http://www.tei-c.org/">Text Encoding Initiatve</a>.</p>
    </div><div id="IntroEpigraphers-TEI">
        <h2>TEI for Epigraphers: What is it, why use it?</h2>
        <p>The Text Encoding Initiative is a research effort aimed at defining an encoding language that encompasses the needs of humanities scholars
            very generally. There are two essential goals motivating the development of the TEI. The first is to enable scholars to represent their
            research materials in digital form using a descriptive language that mirrors the kinds of analytical terms and concepts that are familiar
            and essential to humanistic scholarship. The second goal is to enable scholars to share the resulting materials intelligibly, by using a
            common descriptive language. </p>
        <p>We can think of the TEI encoding language as resembling a human language: a core of shared terms at the center, surrounded by less widely
            shared vocabularies including local usage, specialized terminology, and other variations. At the core of the TEI are the common terms and
            concepts that are broadly shared by scholars in most disciplines: features like paragraphs, generic textual divisions, headings, lists,
            and so forth. More specialized elements are grouped together according to their applications: for instance, elements for detailed encoding
            of names, elements for representing features of manuscripts, elements for capturing the structure of dictionaries, and so forth. The TEI
            is intentionally organized into modules in this way, so that scholars working in specific disciplinary areas may use only the modules
            relevant to their work, and omit the others. The TEI can thus achieve a great deal of breadth without burdening individual scholars and
            projects with the necessity of mastering a very large domain, much of which is only relevant to other disciplines. On the contrary, the
            TEI encoding language can be very directly targeted at a specific domain or task, and can be limited to what is essential to the
            individual project's work. </p>
        <p>Like a human language, TEI can be used in a way that draws on a rich and nuanced vocabulary, with detailed encoding that describes a great
            many textual phenomena, but it can also be used very simply, using only a few essential concepts that describe only the most basic textual
            facts: sections, headings, paragraphs. The more detailed the encoding, the more one can do with it, but factors such as time, cost,
            available staff, and local expertise may place constraints on the level of detail that is feasible. </p>
        <p>In addition to providing an encoding system that scholars can use in its original state, the TEI also provides a way for scholarly projects
            to define custom versions of the TEI language which include modifications that are necessary to support specific local needs. Because
            these custom versions operate within the overall TEI framework, they can use its broadly shared core of terms and concepts, thereby
            avoiding unnecessary work in reinventing these. And because the TEI provides a common framework for creating and describing
            customizations, these can be shared easily and meaningfully. As a result, groups of scholars in particular disciplines can articulate the
            specific goals and methods which characterize their work, and the differences which distinguish them from others working in related
            fields. Instead of mutually unintelligible approaches, different projects can produce results whose differences result from real
            disagreements rather than simple random divergence. </p>
        <p />
    </div><div id="IntroEpigraphers-epidoc">
        <h2>The EpiDoc Customization: TEI for Epigraphers</h2>
        <p>Within this framework, the <a href="http://epidoc.sf.net">EpiDoc community</a> has been working, since 2000, to develop a custom
            version of the TEI Guidelines to support the particular needs of epigraphers. The idea was launched by <a href="http://www.unc.edu/~thomase/">Tom Elliott</a>, an ancient historian at the University of North Carolina at Chapel Hill; the
            aim is both to make the fullest possible use of the work that has already been done, and also to ensure that texts which happen to be
            inscribed are handled in a manner consistent with that used for other texts, and not distanced ffrom them. The EpiDoc customization
            removes irrelevant elements from the main body of the TEI, and it adds provisions for the specific kinds of transcription, analysis, and
            metadata that are essential for epigraphic work. The result is a simple yet powerful language which can be used to mark all of the
            significant features of inscriptions and also represent the accompanying information about the epigraphic object itself.</p>
        <p>To accompany the EpiDoc encoding language, the EpiDoc community has also produced a set of <a title="EpiDoc Resources" href="resources.shtml">software tools</a>,
            as well as <a title="EpiDoc Guidelines" href="resources.shtml#guidelines">documentation</a> which describes how to use the encoding language, the tools, and the other
            elements of the EpiDoc method. The goal is to establish a framework that is easy to learn and use, even for scholars with no technical
            background or support. This may sound improbable, but the enterprise is of the same order as learning to mark-up a standard epigraphic
            text, with the existing series of sigla.</p>
        <p>The group has worked to develop expressions for all the <a title="Epigraphic Conventions" href="#IntroEpigraphers-conventions">agreed epigraphic conventions</a>. They have drawn
            up a set of <a title="EpiDoc Guidelines" href="resources.shtml#guidelines">guidelines</a>, which analyse and present the various fields which may be presented in an
            epigraphic publication, including: <ul>
                <li>a description of the text and/or monument</li>
                <li>the edition of the epigraphic text itself (xref)</li>
                <li>a translation of the text (xref)</li>
                <li>a scholarly commentary related to the text and its unique problems and items of interest(xref)</li>
                <li>a history of the discovery, documentation, and interpretation of the text leading to its present publication (xref)</li>
                <li>a bibliography relevant to this text (xref)</li>
            </ul> See further: <a title="Document Structure section of the EpiDoc Guidelines" href="http://www.stoa.org/projects/epidoc/stable/guidelines/documentstructure.html">Document Structure</a>. </p>
        <p>Further areas under active exploration include developing interoperability. A software tool for converting texts in normal epigraphic
            markup into EpiDoc XML has already been developed (the so-called <a title="The Chapel Hill Electronic Text Converter" href="resources.shtml#chetc">Chapel Hill Electronic Text Converter
            (CHETC)</a>). Other areas involve the use of authoritative lexica. For example, the Inscriptions of Aphrodisias project is working
            closely with the <a href="http://www.lgpn.ox.ac.uk/">Lexicon of Greek Personal Names</a>, to ensure full coverage and consistent
            usage. </p>
        <p>The work, led by Dr. Elliott, has been undertaken by various individual scholars working in close collaboration, and in regular contact
            with the wider profession. They have drawn on the experience of an established EpiDoc project, the <a href="http://vindolanda.csad.ox.ac.uk/exhibition/">Vindolanda Tablets on line</a>, and two current projects: the <a href="http://dev.stg.brown.edu/projects/USEpigraphy/">US Epigraphy Project (USEP)</a> (supported by Brown, Princeton and Rutgers
            Universities), and the <a href="http://insaph.kcl.ac.uk">Inscriptions of Aphrodisias Project (InsAph)</a> (supported by the <a href="http://www.ahrc.ac.uk">Arts and Humanities Research Council</a>). The generous support of the AHRC allowed for the <span class="internal-link">intensive workshop in March 2006</span> where these guidelines were refined. </p>
    </div><div id="footer"><p>Content on this page by Charlotte Roueché and Julia Flanders. Last update: 2006/05/04 22:44:29.<br />Copyright 2006 by the authors.<br />
                        This document derives from content in the <a href="resources.shtml#guidelines">EpiDoc Guidelines</a> (source file: IntroEpigraphers.xml).</p></div></div><!--#include virtual="meta.html"--></div></body></html>